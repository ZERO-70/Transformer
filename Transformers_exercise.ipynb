{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT4Fa3Pk24vBjSO5Bjj8T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZERO-70/Transformer/blob/main/Transformers_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedder for tokens"
      ],
      "metadata": {
        "id": "E4N0tuPb0V3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omu774FPyj0e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class Embedder(nn.Module):\n",
        "  def __init__(self, vocab_size:int, d_model:int):\n",
        "    super.__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x)* math.sqrt(d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Encoder for Embeddings"
      ],
      "metadata": {
        "id": "C15ytG8pqhOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self,d_model,max_seq_len):\n",
        "    super.__init__()\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "    pe = torch.zeros(max_seq_len,d_model)\n",
        "    positions = torch.arange(max_seq_len).float().unsqueeze(1)\n",
        "    denominator = torch.exp(\n",
        "        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "    )\n",
        "    pe[:, 0::2] = torch.sin(positions * denominator)\n",
        "    pe[:, 1::2] = torch.cos(positions * denominator)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer(\"pe\", pe)"
      ],
      "metadata": {
        "id": "FxG3XIN7jJmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position = torch.arange(0, 12, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "print(position)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXezv3W2206K",
        "outputId": "2d2ac0c1-040f-48c9-f88d-a5536d21d3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 3.],\n",
            "        [ 4.],\n",
            "        [ 5.],\n",
            "        [ 6.],\n",
            "        [ 7.],\n",
            "        [ 8.],\n",
            "        [ 9.],\n",
            "        [10.],\n",
            "        [11.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MultiHeadAttention"
      ],
      "metadata": {
        "id": "9WJ68JFQHl71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.QueryLinear = nn.Linear(d_model, d_model, bias = False)\n",
        "    self.KeyLinear = nn.Linear(d_model, d_model, bias = False)\n",
        "    self.ValueLinear = nn.Linear(d_model, d_model, bias = False)\n",
        "    self.FinalLinear = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "  def split_input_into_heads(self,x,batch_size):\n",
        "    seq_lenght = x.size(1)\n",
        "    x = x.reshape(batch_size, seq_lenght, self.num_heads, self.head_dim)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def compute_attention(self,query,key,value,mask=None):\n",
        "    scores = torch.matmul(query,key.transpose(-2,-1))/(self.head_dim ** 0.5)\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attention_weights = torch.softmax(scores,dim=-1)\n",
        "    return torch.matmul(attention_weights,value)\n",
        "\n",
        "  def combine_attention(self,x,batch_size):\n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "    # -1 parameter means to figure out the dimesion\n",
        "    x = x.reshape(batch_size, -1, self.d_model)\n",
        "    return x\n",
        "\n",
        "  def forward(self,query,key,value,mask = None):\n",
        "    batch_size = query.size(0)\n",
        "    query = self.split_input_into_heads(self.QueryLinear(query),batch_size)\n",
        "    key = self.split_input_into_heads(self.KeyLinear(key),batch_size)\n",
        "    value = self.split_input_into_heads(self.ValueLinear(value),batch_size)\n",
        "    attention = self.compute_attention(query,key,value,mask)\n",
        "    reorderd_attention = self.combine_attention(attention,batch_size)\n",
        "    return self.FinalLinear(reorderd_attention)\n"
      ],
      "metadata": {
        "id": "-7bm4lcIINxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FeedForward"
      ],
      "metadata": {
        "id": "LAvdPldgvB-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_ff):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model,d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff,d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    return self.linear2(self.relu(self.linear1(x)))"
      ],
      "metadata": {
        "id": "WgS1-RchvE8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder"
      ],
      "metadata": {
        "id": "87giUojrspsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,num_heads,d_ff,drop_out):\n",
        "    super().__init__()\n",
        "    self.attention_block = MultiHeadAttention(d_model,num_heads)\n",
        "    self.feed_block = FeedForward(d_model,d_ff)\n",
        "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "    self.drop_out = nn.Dropout(drop_out)\n",
        "  def forward(self,x,mask):\n",
        "    attention_output = self.attention_block(x,x,x,mask)\n",
        "    x = self.layer_norm1(x + self.drop_out(attention_output))\n",
        "    feed_output = self.feed_block(x)\n",
        "    return self.layer_norm2(x + self.drop_out(feed_output))"
      ],
      "metadata": {
        "id": "sY6O36hi0giH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wraper for Multiple Encoders"
      ],
      "metadata": {
        "id": "xV7m4K9Fssvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,num_layers,num_heads,d_ff,drop_out,max_seq_len):\n",
        "    super().__init__()\n",
        "    self.embedder = Embedder(vocab_size,d_model)\n",
        "    self.positional_encoder = PositionalEncoder(d_model,max_seq_len)\n",
        "    self.encoder_layers = nn.ModuleList([Encoder(d_model,num_heads,d_ff,drop_out) for _ in range(num_layers)])\n",
        "    self.drop_out = nn.Dropout(drop_out)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    x = self.embedder(x)\n",
        "    x = self.positional_encoder(x)\n",
        "    for layer in self.encoder_layers:\n",
        "      x = layer(x,mask)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1qMORvBC34f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifier"
      ],
      "metadata": {
        "id": "zVGRJmZHs1TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class ClassifierHead(nn.Module):\n",
        "  def __init__(self, d_model,num_classes):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(d_model,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    logits = self.linear(x)\n",
        "    return F.log_softmax(logits,dim = -1)\n"
      ],
      "metadata": {
        "id": "-A-Cn2RrByw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKWWRl0BsoYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}